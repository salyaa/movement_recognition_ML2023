{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - ML\n",
    "### Run NN : Impact of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from processing import *\n",
    "import seaborn as sns\n",
    "from error_impact_functions import neural_network_corr, neural_network_err\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'All_Relative_Results_Cleaned.parquet'\n",
    "data = pd.read_parquet(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27187,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_with_nan = data[data.isnull().any(axis=1)]\n",
    "\n",
    "def percentage_nan_by_row(df):\n",
    "    return df.isnull().mean(axis=1)*100\n",
    "\n",
    "percentage_nan_by_rows = percentage_nan_by_row(rows_with_nan.drop(['Participant', 'Set', 'Camera','Exercise', 'time(s)'], axis='columns'))\n",
    "percentage_nan_by_rows[percentage_nan_by_rows==100].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that all rows with NaNs values have more than 99% of missing numerical values. Hence we can remove this rows from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data.dropna().copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We create our Machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see the influence of error on the collected data. To see if their impact is relevant, we base our training set only on correctly executed exercises and then test our prediction algorithm on the correct data set and then on the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corr = data_cleaned[data_cleaned[\"Set\"] == \"Correct\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we reduce the dimension of the correct data set, to only focus on a few parts of detectors. We decide to keep : ankles, wrists, hips, knees, elbows, shoulders and nose as we consider them the most significant. This keeps 44 columns out of 104."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['Participant','Exercise','Set','Camera','time(s)',\n",
    "                     'left_ankle_x','left_ankle_y','left_ankle_z',\n",
    "                     'right_ankle_x','right_ankle_y','right_ankle_z',\n",
    "                     'left_wrist_x','left_wrist_y','left_wrist_z',\n",
    "                     'right_wrist_x','right_wrist_y','right_wrist_z',\n",
    "                     'left_hip_x','left_hip_y','left_hip_z',\n",
    "                     'right_hip_x','right_hip_y','right_hip_z',\n",
    "                     'left_knee_x','left_knee_y','left_knee_z',\n",
    "                     'right_knee_x','right_knee_y','right_knee_z',\n",
    "                     'left_elbow_x','left_elbow_y','left_elbow_z',\n",
    "                     'right_elbow_x','right_elbow_y','right_elbow_z',\n",
    "                     'left_shoulder_x','left_shoulder_y','left_shoulder_z',\n",
    "                     'right_shoulder_x','right_shoulder_y','right_shoulder_z',\n",
    "                     'nose_x', 'nose_y', 'nose_z'\n",
    "                     ]\n",
    "\n",
    "data_corr_reduc = data_corr[columns_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our NN model on the correct dataset X_corr, Y_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Goldruush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11877/11877 [==============================] - 128s 11ms/step - loss: 0.2160 - accuracy: 0.9144 - val_loss: 0.1429 - val_accuracy: 0.9404\n",
      "Epoch 2/10\n",
      "11877/11877 [==============================] - 75s 6ms/step - loss: 0.1652 - accuracy: 0.9314 - val_loss: 0.1312 - val_accuracy: 0.9448\n",
      "Epoch 3/10\n",
      "11877/11877 [==============================] - 40s 3ms/step - loss: 0.1534 - accuracy: 0.9361 - val_loss: 0.1260 - val_accuracy: 0.9476\n",
      "Epoch 4/10\n",
      "11877/11877 [==============================] - 38s 3ms/step - loss: 0.1462 - accuracy: 0.9388 - val_loss: 0.1195 - val_accuracy: 0.9486\n",
      "Epoch 5/10\n",
      "11877/11877 [==============================] - 37s 3ms/step - loss: 0.1421 - accuracy: 0.9403 - val_loss: 0.1151 - val_accuracy: 0.9515\n",
      "Epoch 6/10\n",
      "11877/11877 [==============================] - 37s 3ms/step - loss: 0.1392 - accuracy: 0.9419 - val_loss: 0.1160 - val_accuracy: 0.9500\n",
      "Epoch 7/10\n",
      "11877/11877 [==============================] - 37s 3ms/step - loss: 0.1362 - accuracy: 0.9429 - val_loss: 0.1124 - val_accuracy: 0.9527\n",
      "Epoch 8/10\n",
      "11877/11877 [==============================] - 38s 3ms/step - loss: 0.1334 - accuracy: 0.9437 - val_loss: 0.1111 - val_accuracy: 0.9525\n",
      "Epoch 9/10\n",
      "11877/11877 [==============================] - 38s 3ms/step - loss: 0.1326 - accuracy: 0.9445 - val_loss: 0.1113 - val_accuracy: 0.9504\n",
      "Epoch 10/10\n",
      "11877/11877 [==============================] - 39s 3ms/step - loss: 0.1315 - accuracy: 0.9446 - val_loss: 0.1095 - val_accuracy: 0.9519\n",
      "11877/11877 [==============================] - 21s 2ms/step\n",
      "Test Accuracy: 95.19%\n",
      "F1 Score macro: 0.9549\n",
      "F1 Score micro: 0.9519\n"
     ]
    }
   ],
   "source": [
    "# Define X_corr as coordinates of correct sets\n",
    "X_corr = data_corr_reduc.drop(['Participant', 'Set', 'Camera', 'Exercise'], axis=1)\n",
    "\n",
    "# Define Y_corr as exercise column of the correct sets\n",
    "Y_corr = data_corr_reduc['Exercise']\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_corr_norm = scaler.fit_transform(X_corr)\n",
    "\n",
    "# Train-test split\n",
    "X_train_corr, X_test_corr, y_train_corr, y_test_corr = train_test_split(X_corr_norm, Y_corr, train_size=0.5, test_size=0.5, random_state=42)\n",
    "\n",
    "# Encode the target variable (assuming 'exercise' is your target variable)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded_corr = label_encoder.fit_transform(y_train_corr)\n",
    "y_test_encoded_corr = label_encoder.transform(y_test_corr)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded_onehot_corr = onehot_encoder.fit_transform(y_train_encoded_corr.reshape(-1, 1))\n",
    "y_test_encoded_onehot_corr = onehot_encoder.transform(y_test_encoded_corr.reshape(-1, 1))\n",
    "\n",
    "unique_exercises = Y_corr.unique()\n",
    "\n",
    "num_classes = len(unique_exercises)\n",
    "\n",
    "# Neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_corr.shape[1],)),\n",
    "    Dropout(0.5),  # Adding dropout for regularization\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with one-hot encoded labels\n",
    "model.fit(X_train_corr, y_train_encoded_corr, epochs=10, validation_data=(X_test_corr, y_test_encoded_corr))\n",
    "\n",
    "# Get the predicted probabilities for each class\n",
    "y_pred_probs_corr = model.predict(X_test_corr)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred_corr = np.argmax(y_pred_probs_corr, axis=1)\n",
    "\n",
    "# Assuming y_test_encoded_onehot is the true labels in one-hot encoded form\n",
    "# Convert one-hot encoded labels to integers\n",
    "y_test_corr = np.argmax(y_test_encoded_onehot_corr, axis=1)\n",
    "\n",
    "# Evaluate the model on the test set with categorical_crossentropy\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Convert one-hot encoded labels to integers\n",
    "y_true_corr = np.argmax(y_test_encoded_onehot_corr, axis=1)\n",
    "\n",
    "accuracy = np.sum(y_true_corr == y_pred_corr) / len(y_true_corr)\n",
    "f1_score_NN_macro = f1_score(y_true_corr, y_pred_corr, average='macro')\n",
    "f1_score_NN_micro = f1_score(y_true_corr, y_pred_corr, average='micro')\n",
    "\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'F1 Score macro: {f1_score_NN_macro:.4f}')\n",
    "print(f'F1 Score micro: {f1_score_NN_micro:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our full dataset as correct and incorrect sets and reduce the dimension as we did before :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_with_err = data_cleaned[columns_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test the trained model on the complete dataset containing error and observe the impact on accuracy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13647/13647 [==============================] - 17s 1ms/step\n",
      "13647/13647 [==============================] - 22s 2ms/step - loss: 0.1742 - accuracy: 0.9348\n",
      "Test Accuracy: 93.48%\n",
      "F1 Score macro: 0.9346\n",
      "F1 Score micro: 0.9348\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each group\n",
    "X_err = set_with_err.drop(['Participant', 'Set', 'Camera', 'Exercise'], axis=1)\n",
    "\n",
    "# Apply the function to each group and concatenate the results\n",
    "Y_err = set_with_err['Exercise']\n",
    "\n",
    "# Normalize data\n",
    "X_normalized_err = scaler.fit_transform(X_err)\n",
    "\n",
    "# Train-test split\n",
    "X_train_err, X_test_err, y_train_err, y_test_err = train_test_split(X_normalized_err, Y_err, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the target variable (assuming 'exercise' is your target variable)\n",
    "y_test_encoded_err = label_encoder.transform(y_test_err)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "y_test_encoded_onehot_err = onehot_encoder.transform(y_test_encoded_err.reshape(-1, 1))\n",
    "\n",
    "# Get the predicted probabilities for each row \n",
    "y_pred_probs_err = model.predict(X_test_err)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred_err = np.argmax(y_pred_probs_err, axis=1)\n",
    "\n",
    "# Assuming y_test_encoded_onehot is the true labels in one-hot encoded form\n",
    "# Convert one-hot encoded labels to integers\n",
    "y_test_err = np.argmax(y_test_encoded_onehot_err, axis=1)\n",
    "\n",
    "# Evaluate the model on the test set with categorical_crossentropy\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "test_loss_err, test_accuracy_err = model.evaluate(X_test_err, y_test_encoded_onehot_err)\n",
    "\n",
    "# Convert one-hot encoded labels to integers\n",
    "y_true_err = np.argmax(y_test_encoded_onehot_err, axis=1)\n",
    "\n",
    "accuracy_err = np.sum(y_true_err == y_pred_err) / len(y_true_err)\n",
    "f1_score_NN_macro_err = f1_score(y_true_err, y_pred_err, average='macro')\n",
    "f1_score_NN_micro_err = f1_score(y_true_err, y_pred_err, average='micro')\n",
    "\n",
    "print(f'Test Accuracy: {accuracy_err * 100:.2f}%')\n",
    "print(f'F1 Score macro: {f1_score_NN_macro_err:.4f}')\n",
    "print(f'F1 Score micro: {f1_score_NN_micro_err:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion for this error study, we notice that the accuracy over the correct dataset is 95.19%, as the accuracy over the full dataset using the same weights obtained after training only over correct sample gives a score of 93.48%. This might give an idea of how introducing a new error that was not taken into acount during training impact the prediction accuracy. This difference is quite small and we can rely on the robustness of Neural Networks to deal with new errors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
